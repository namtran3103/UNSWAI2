{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports and global declarations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z5629534, Hoang-Nam Tran\n",
    "\n",
    "import random \n",
    "from env import StaticGridEnv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import utils\n",
    "\n",
    "#seeding for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "#environment variables\n",
    "numActions = 4\n",
    "validActions = [0, 1, 2, 3]\n",
    "numStates = 10*10 #10x10 grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining methods for metrics calculation/display**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCumulativeReward(cumulativeReward):\n",
    "    \n",
    "    #Plotting cumulative reward (y-axis) vs episodes (x-axis)\n",
    "    \n",
    "    plt.plot(cumulativeReward)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.title('Cumulative Reward vs Episodes')\n",
    "    plt.show()\n",
    "\n",
    "def calculateSuccessRate(numSuccess, numEpisodes):\n",
    "    \n",
    "    #Calculating success rate\n",
    "    \n",
    "    successRate = (numSuccess / numEpisodes) * 100\n",
    "    return successRate\n",
    "    \n",
    "    \n",
    "def calculateAverageLearningSpeed(stepsPerEpisode):\n",
    "    \n",
    "    #Calculating average learning speed\n",
    "    \n",
    "    averageLearningSpeed = 1 / np.mean(stepsPerEpisode)\n",
    "    return averageLearningSpeed\n",
    "\n",
    "def calculateAverageRewardPerEpisode(totalRewardsPerEpisode):\n",
    "    \n",
    "    #Calculating average reward per episode\n",
    "    \n",
    "    averageRewardPerEpisode = np.mean(totalRewardsPerEpisode)\n",
    "    return averageRewardPerEpisode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1: Training agent with Q-learning using epsilon-greedy method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Q-learning environment\n",
    "envQ = StaticGridEnv(42)\n",
    "\n",
    "# Initialize the Q-table, states are 1D\n",
    "q = np.zeros((numStates, numActions))\n",
    "\n",
    "# Params for Q-learning\n",
    "epsilonQ = 0.1\n",
    "alphaQ = 0.8    # Learning rate\n",
    "gammaQ = 0.95   # Discount factor\n",
    "\n",
    "# Constraints\n",
    "numEpisodesQ = 1000\n",
    "maxStepsPerEpisodeQ = 100\n",
    "\n",
    "\n",
    "# Cumulative metrics\n",
    "totalRewardsPerEpisodeQ = []\n",
    "stepsPerEpisodeQ = []\n",
    "successfulEpisodesQ = 0\n",
    "\n",
    "for episodeQ in range(numEpisodesQ):\n",
    "    \n",
    "    # Reset the environment and metrics for the episode, flatten the 2D state to 1D\n",
    "    twoDimStateQ = envQ.reset()\n",
    "    stateQ = twoDimStateQ[0]*10 + twoDimStateQ[1]\n",
    "    doneQ = False\n",
    "    totalRewardsQ = 0\n",
    "    stepsQ = 0\n",
    "    actionQ = 0\n",
    "    \n",
    "    \n",
    "    for stepQ in range(maxStepsPerEpisodeQ):\n",
    "        # Choose an action (epsilon-greedy policy)\n",
    "        if random.uniform(0, 1) > epsilonQ: # Exploit\n",
    "            actionQ =  np.argmax(q[stateQ, :])\n",
    "        else: # Explore\n",
    "            actionQ = random.choice(validActions)\n",
    "\n",
    "        # Take the action\n",
    "        nextStateTwoDimQ, rewardQ, doneQ, _ = envQ.step(actionQ)\n",
    "        nextStateQ = nextStateTwoDimQ[0]*10 + nextStateTwoDimQ[1]\n",
    "\n",
    "        # Update Q-table\n",
    "        q[stateQ, actionQ] = q[stateQ, actionQ] + alphaQ * (rewardQ + gammaQ * np.max(q[nextStateQ, :]) - q[stateQ, actionQ])\n",
    "\n",
    "        # Update metrics\n",
    "        totalRewardsQ += rewardQ\n",
    "        stepsQ += 1\n",
    "\n",
    "        # Render the environment (optional)\n",
    "        #envQ.render(episode=episodeQ, learning_type=\"Q-learning\")\n",
    "\n",
    "        # Transition to the next state\n",
    "        twoDimStateQ = nextStateTwoDimQ\n",
    "        stateQ = nextStateQ\n",
    "\n",
    "        if doneQ:\n",
    "            successfulEpisodesQ += 1\n",
    "            break\n",
    "\n",
    "    # Update cumulative metrics\n",
    "    totalRewardsPerEpisodeQ.append(totalRewardsQ)\n",
    "    stepsPerEpisodeQ.append(stepsQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance metrics and plotting cumulative reward (Q-learning)**\n",
    "\n",
    "**Saving metrics in JSON-file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "successRateQ = calculateSuccessRate(successfulEpisodesQ, numEpisodesQ)\n",
    "averageRewardPerEpQ = calculateAverageRewardPerEpisode(totalRewardsPerEpisodeQ)\n",
    "averageLearningSpeedQ = calculateAverageLearningSpeed(stepsPerEpisodeQ)\n",
    "\n",
    "print(\"Agent 1 – Q-Learning\")\n",
    "\n",
    "print(\"Success Rate (%): \", successRateQ)\n",
    "print(\"Average Reward per Episode: \", averageRewardPerEpQ)\n",
    "print(\"Average Learning Speed: \", averageLearningSpeedQ)\n",
    "\n",
    "# Save performance metrics to a JSON file\n",
    "metrics = {\n",
    "    \"Success Rate (%)\": successRateQ,\n",
    "    \"Average Reward per Episode\": averageRewardPerEpQ,\n",
    "    \"Average Learning Speed\": averageLearningSpeedQ,\n",
    "}\n",
    "\n",
    "with open(\"metricsQ.json\", \"w\") as fQ:\n",
    "    json.dump(metrics, fQ, indent=4)\n",
    "\n",
    "# Plot cumulative reward vs episodes\n",
    "plotCumulativeReward(totalRewardsPerEpisodeQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2: Training agent with SARSA using epsilon-greedy method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the SARSA environment\n",
    "envS = StaticGridEnv(42)\n",
    "\n",
    "# Initialize the Q-table, states are 1D\n",
    "sarsa = np.zeros((numStates, numActions))\n",
    "\n",
    "\n",
    "# Params\n",
    "epsilonS = 0.1\n",
    "alphaS = 0.9    # Learning rate\n",
    "gammaS = 0.8   # Discount factor\n",
    "\n",
    "# Constraints\n",
    "maxEpisodesS = 1000\n",
    "maxStepsPerEpisodeS = 100\n",
    "\n",
    "\n",
    "# Cumulative metrics\n",
    "totalRewardsPerEpisodeS = []\n",
    "stepsPerEpisodeS = []\n",
    "successfulEpisodesS = 0\n",
    "\n",
    "for episodeS in range(maxEpisodesS):\n",
    "    \n",
    "    # Reset the environment and metrics for the episode, flatten the 2D state to 1D\n",
    "    twoDimStateS = envS.reset()\n",
    "    stateS = twoDimStateS[0]*10 + twoDimStateS[1]\n",
    "    doneS = False\n",
    "    totalRewardsS = 0\n",
    "    stepsS = 0\n",
    "    actionS = 0\n",
    "    nextActionS = 0\n",
    "    \n",
    "    \n",
    "    # Choose an action (epsilon-greedy policy)\n",
    "    if random.uniform(0, 1) > epsilonS: # Exploit\n",
    "        actionS =  np.argmax(sarsa[stateS, :])\n",
    "    else: # Explore\n",
    "        actionS = random.choice(validActions)\n",
    "    \n",
    "    for stepS in range(maxStepsPerEpisodeS):\n",
    "        \n",
    "        # Take the action\n",
    "        nextStateTwoDimS, rewardS, doneS, _ = envS.step(actionS)\n",
    "        nextStateS = nextStateTwoDimS[0]*10 + nextStateTwoDimS[1]\n",
    "        \n",
    "        # Choose next action (epsilon-greedy policy)\n",
    "        if random.uniform(0, 1) > epsilonS: # Exploit\n",
    "            nextActionS =  np.argmax(sarsa[nextStateS, :])\n",
    "        else: # Explore\n",
    "            nextActionS = random.choice(validActions)\n",
    "\n",
    "        # Update Q-table\n",
    "        sarsa[stateS, actionS] = sarsa[stateS, actionS] + alphaS * (rewardS + gammaS * sarsa[nextStateS, nextActionS] - sarsa[stateS, actionS])\n",
    "\n",
    "        # Update metrics\n",
    "        totalRewardsS += rewardS\n",
    "        stepsS += 1\n",
    "\n",
    "        # Render the environment (optional)\n",
    "        #envS.render(episode=episodeS, learning_type=\"Q-learning\")\n",
    "\n",
    "        # Transition to the next state and action\n",
    "        twoDimStateS = nextStateTwoDimS\n",
    "        stateS = nextStateS\n",
    "        actionS = nextActionS\n",
    "\n",
    "        if doneS:\n",
    "            successfulEpisodesS += 1\n",
    "            break\n",
    "\n",
    "    # Update cumulative metrics\n",
    "    totalRewardsPerEpisodeS.append(totalRewardsS)\n",
    "    stepsPerEpisodeS.append(stepsS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Performance metrics and plotting cumulative reward (SARSA)**\n",
    "\n",
    "**Saving metrics in JSON-file**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance metrics\n",
    "successRateS = calculateSuccessRate(successfulEpisodesS, maxEpisodesS)\n",
    "averageRewardPerEpisodeS = calculateAverageRewardPerEpisode(totalRewardsPerEpisodeS)\n",
    "averageLearningSpeedS = calculateAverageLearningSpeed(stepsPerEpisodeS)\n",
    "\n",
    "print(\"Agent 2 – SARSA\")\n",
    "\n",
    "print(\"Success Rate (%): \", successRateS)\n",
    "print(\"Average Reward per Episode: \", averageRewardPerEpisodeS)\n",
    "print(\"Average Learning Speed: \", averageLearningSpeedS)\n",
    "\n",
    "\n",
    "# Save performance metrics to a JSON file\n",
    "metricsS = {\n",
    "    \"Success Rate (%)\": successRateS,\n",
    "    \"Average Reward per Episode\": averageRewardPerEpisodeS,\n",
    "    \"Average Learning Speed\": averageLearningSpeedS,\n",
    "}\n",
    "\n",
    "with open(\"metricsS.json\", \"w\") as fS:\n",
    "    json.dump(metricsS, fS, indent=4)\n",
    "    \n",
    "# Plot cumulative reward vs episodes\n",
    "plotCumulativeReward(totalRewardsPerEpisodeS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing Teacher Feedback Mechanism with possible values for availability/accuracy according to given assignment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provideTeacherAdvice(task1, currentState, availability, accuracy):\n",
    "    \n",
    "    # Choose teacher trained with Q-learning (Task1) or SARSA (Task2)\n",
    "    # task1: True for Q-learning, False for SARSA\n",
    "    if task1:\n",
    "        teacherTable = q\n",
    "    else:\n",
    "        teacherTable = sarsa\n",
    "    \n",
    "    # In case of correct advice, choose the action with the highest Q-value    \n",
    "    correctAction = np.argmax(teacherTable[currentState, :])\n",
    "    \n",
    "    # In case of incorrect advice, choose a random action from the remaining actions\n",
    "    otherActions = [0, 1, 2, 3]\n",
    "    otherActions.remove(correctAction)\n",
    "    possibleAction = random.choice(otherActions)\n",
    "    \n",
    "    if random.uniform(0, 1) < availability:\n",
    "        if random.uniform(0, 1) < accuracy:\n",
    "            # Return correct advice\n",
    "            return correctAction\n",
    "        else:\n",
    "            # Return incorrect advice\n",
    "            return possibleAction\n",
    "    else:\n",
    "        # Return no advice (-1 is not valid action)\n",
    "        return -1\n",
    "    \n",
    "\n",
    "availability = [0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "accuracy = [0.2, 0.4, 0.6, 0.8, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3: Defining method to train student using the agent trained in Task 1 (Q-learning) as teacher**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a is availability, b is accuracy\n",
    "\n",
    "# Initialize the Q-table for the student, states are 1D\n",
    "studentQ = np.zeros((numStates, numActions))\n",
    "\n",
    "# Initialize the data frame to store the performance metrics\n",
    "dataFrameQ = pd.DataFrame(columns=['Availability', 'Accuracy', 'Avg Reward', 'Success Rate (%)', 'Avg Learning Speed'])\n",
    "\n",
    "# Initialize the data frame for the heatmap, accuracy is y-axis => index, availability is x-axis => columns\n",
    "dataFrameHeatQ = pd.DataFrame(index=accuracy, columns=availability)\n",
    "\n",
    "\n",
    "def studentQTraining(a, b):\n",
    "    # Cumulative metrics\n",
    "    successfulEpStudentQ = 0\n",
    "    totalRewPerEpStudentQ = []\n",
    "    stepsPerEpStudentQ = []\n",
    "    \n",
    "    for episodeStudQ in range(numEpisodesQ):\n",
    "        \n",
    "        # Reset the environment and metrics for the episode, flatten the 2D state to 1D\n",
    "        twoDimStateStudQ = envQ.reset()\n",
    "        stateStudQ = twoDimStateStudQ[0]*10 + twoDimStateStudQ[1]\n",
    "        doneStudQ = False\n",
    "        totalRewardsStudQ = 0\n",
    "        stepsStudQ = 0\n",
    "        actionStudQ = 0\n",
    "            \n",
    "        for stepStudQ in range(maxStepsPerEpisodeQ):\n",
    "            # Ask teacher for advice, param task1 = True for teacher from Task1\n",
    "            actionStudQ = provideTeacherAdvice(True, stateStudQ, a, b)\n",
    "            \n",
    "            # If no advice is provided, choose an action with epsilon-greedy policy\n",
    "            if actionStudQ == -1:\n",
    "                if random.uniform(0, 1) > epsilonQ: # Exploit\n",
    "                    actionStudQ =  np.argmax(studentQ[stateStudQ, :])\n",
    "                else: # Explore\n",
    "                    actionStudQ = random.choice(validActions)\n",
    "                \n",
    "            # Take the action\n",
    "            nextStateTwoDimStudQ, rewardStudQ, doneStudQ, _ = envQ.step(actionStudQ)\n",
    "            nextStateStudQ = nextStateTwoDimStudQ[0]*10 + nextStateTwoDimStudQ[1]\n",
    "\n",
    "            # Update student Q-table\n",
    "            studentQ[stateStudQ, actionStudQ] = studentQ[stateStudQ, actionStudQ] + alphaQ * \\\n",
    "                (rewardStudQ + gammaQ * np.max(studentQ[nextStateStudQ, :]) - studentQ[stateStudQ, actionStudQ])\n",
    "\n",
    "            # Update metrics\n",
    "            totalRewardsStudQ += rewardStudQ\n",
    "            stepsStudQ += 1\n",
    "            \n",
    "\n",
    "            # Transition to the next state\n",
    "            twoDimStateStudQ = nextStateTwoDimStudQ\n",
    "            stateStudQ = nextStateStudQ\n",
    "\n",
    "            if doneStudQ:\n",
    "                successfulEpStudentQ += 1\n",
    "                break\n",
    "\n",
    "        # Update cumulative metrics\n",
    "        totalRewPerEpStudentQ.append(totalRewardsStudQ)\n",
    "        stepsPerEpStudentQ.append(stepsStudQ)\n",
    "\n",
    "    \n",
    "    # Calculate performance metrics    \n",
    "    successRateStudQ = calculateSuccessRate(successfulEpStudentQ, numEpisodesQ)\n",
    "    averageRewardPerEpStudQ = calculateAverageRewardPerEpisode(totalRewPerEpStudentQ)\n",
    "    averageLearningSpeedStudQ = calculateAverageLearningSpeed(stepsPerEpStudentQ)\n",
    "    \n",
    "    # Save performance metrics to the data frame\n",
    "    dataFrameQ.loc[len(dataFrameQ)] = [a, b, averageRewardPerEpStudQ, successRateStudQ, averageLearningSpeedStudQ]\n",
    "    \n",
    "    # Save average reward to heatmap df, flip b and a because index is accuracy and column is availability\n",
    "    dataFrameHeatQ.loc[b,a] = averageRewardPerEpStudQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conducting teacher-assisted training for all specified availability/accuracy values (Q-learning)**\n",
    "\n",
    "**Saving metrics, displaying DataFrame, and plotting the avg reward heatmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all combinations of availability and accuracy\n",
    "for a in availability:\n",
    "    for b in accuracy:\n",
    "        studentQTraining(a, b)\n",
    "\n",
    "# Close the Q-learning environment\n",
    "envQ.close()\n",
    "\n",
    "# Save the dataframe to csv\n",
    "dataFrameQ.to_csv('dataFrameQlearning.csv', index=False)\n",
    "\n",
    "\n",
    "# Convert the heatmap data frame to numeric values for heatmap\n",
    "dataFrameHeatQ = dataFrameHeatQ.apply(pd.to_numeric)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sn.heatmap(dataFrameHeatQ, annot=True, fmt=\".3f\", cmap=\"crest\")\n",
    "plt.title(\"Average Reward for Different Teacher Availability and Accuracy (Q-learning Teacher)\")\n",
    "plt.xlabel(\"Availability\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "display(dataFrameQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4: Defining method to train student using the agent trained in Task 2 (SARSA) as teacher**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a is availability, b is accuracy\n",
    "\n",
    "# Initialize the Q-table for the student, states are 1D\n",
    "studentSarsa = np.zeros((numStates, numActions))\n",
    "\n",
    "# Initialize the data frame to store the performance metrics\n",
    "dataFrameSarsa = pd.DataFrame(columns=['Availability', 'Accuracy', 'Avg Reward', 'Success Rate (%)', 'Avg Learning Speed'])\n",
    "\n",
    "# Initialize the data frame for the heatmap, accuracy is y-axis => index, availability is x-axis => columns\n",
    "dataFrameHeatSarsa = pd.DataFrame(index=accuracy, columns=availability)\n",
    "\n",
    "def studentSarsaTraining(a,b):\n",
    "    # Cumulative metrics\n",
    "    totalRewardsPerEpStudS = []\n",
    "    stepsPerEpStudS = []\n",
    "    successfulEpStudS = 0\n",
    "\n",
    "    for episodeStudS in range(maxEpisodesS):\n",
    "        \n",
    "        # Reset the environment and metrics for the episode, flatten the 2D state to 1D\n",
    "        twoDimStateStudS = envS.reset()\n",
    "        stateStudS = twoDimStateStudS[0]*10 + twoDimStateStudS[1]\n",
    "        doneStudS = False\n",
    "        totalRewardsStudS = 0\n",
    "        stepsStudS = 0\n",
    "        actionStudS = 0\n",
    "        nextActionStudS = 0\n",
    "        \n",
    "        # Ask teacher for advice, param task1 = False for teacher from Task2\n",
    "        actionStudS = provideTeacherAdvice(False, stateStudS, a, b)\n",
    "        \n",
    "        # If no advice is provided, choose an action with epsilon-greedy policy\n",
    "        if actionStudS == -1:\n",
    "            if random.uniform(0, 1) > epsilonS: # Exploit\n",
    "                actionStudS =  np.argmax(studentSarsa[stateStudS, :])\n",
    "            else: # Explore\n",
    "                actionStudS = random.choice(validActions)\n",
    "        \n",
    "        for stepStudS in range(maxStepsPerEpisodeS):\n",
    "            \n",
    "\n",
    "            # Take the action\n",
    "            nextStateTwoDimStudS, rewardStudS, doneStudS, _ = envS.step(actionStudS)\n",
    "            nextStateStudS = nextStateTwoDimStudS[0]*10 + nextStateTwoDimStudS[1]\n",
    "            \n",
    "            \n",
    "            # Ask teacher for advice on the next action\n",
    "            nextActionStudS = provideTeacherAdvice(False, nextStateStudS, a, b)\n",
    "            \n",
    "            # If no advice is provided, choose an action (epsilon-greedy policy)\n",
    "            if nextActionStudS == -1:\n",
    "                if random.uniform(0, 1) > epsilonS: # Exploit\n",
    "                    nextActionStudS =  np.argmax(studentSarsa[nextStateStudS, :])\n",
    "                else: # Explore\n",
    "                    nextActionStudS = random.choice(validActions)\n",
    "\n",
    "            # Update Q-table\n",
    "            studentSarsa[stateStudS, actionStudS] = studentSarsa[stateStudS, actionStudS] + \\\n",
    "                alphaS * (rewardStudS + gammaS * studentSarsa[nextStateStudS, nextActionStudS] - studentSarsa[stateStudS, actionStudS])\n",
    "\n",
    "            # Update metrics\n",
    "            totalRewardsStudS += rewardStudS\n",
    "            stepsStudS += 1\n",
    "\n",
    "            # Transition to the next state\n",
    "            twoDimStateStudS = nextStateTwoDimStudS\n",
    "            stateStudS = nextStateStudS\n",
    "            actionStudS = nextActionStudS\n",
    "\n",
    "            if doneStudS:\n",
    "                successfulEpStudS += 1\n",
    "                break\n",
    "\n",
    "        # Track cumulative metrics\n",
    "        totalRewardsPerEpStudS.append(totalRewardsStudS)\n",
    "        stepsPerEpStudS.append(stepsStudS)\n",
    "        \n",
    "\n",
    "    # Calculate performance metrics\n",
    "    successRateStudS = calculateSuccessRate(successfulEpStudS, maxEpisodesS)\n",
    "    averageRewardPerEpStudS = calculateAverageRewardPerEpisode(totalRewardsPerEpStudS)\n",
    "    averageLearningSpeedStudS = calculateAverageLearningSpeed(stepsPerEpStudS)\n",
    "    \n",
    "    # Save performance metrics to the data frame\n",
    "    dataFrameSarsa.loc[len(dataFrameSarsa)] = [a, b, averageRewardPerEpStudS, successRateStudS, averageLearningSpeedStudS]\n",
    "    \n",
    "    # Save average reward to heatmap df, flip b and a because index is accuracy and column is availability\n",
    "    dataFrameHeatSarsa.loc[b,a] = averageRewardPerEpStudS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conducting teacher-assisted training for all specified availability/accuracy values (SARSA)**\n",
    "\n",
    "**Saving metrics, displaying DataFrame, and plotting the avg reward heatmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all combinations of availability and accuracy\n",
    "for a in availability:\n",
    "    for b in accuracy:\n",
    "        studentSarsaTraining(a, b)\n",
    "\n",
    "# Close the SARSA environment\n",
    "envS.close()\n",
    "\n",
    "# Save the dataframe to csv\n",
    "dataFrameSarsa.to_csv('dataFrameSarsa.csv', index=False)\n",
    "\n",
    "# Convert the heatmap data frame to numeric values for heatmap\n",
    "dataFrameHeatSarsa = dataFrameHeatSarsa.apply(pd.to_numeric)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sn.heatmap(dataFrameHeatSarsa, annot=True, fmt=\".3f\", cmap=\"crest\")\n",
    "plt.title(\"Average Reward for Different Teacher Availability and Accuracy (SARSA Teacher)\")\n",
    "plt.xlabel(\"Availability\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "\n",
    "display(dataFrameSarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-learning: loading results from Task 1 and 3, performance comparison of teacher mechanism with baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe from Task3\n",
    "dfLoadQ = pd.read_csv('dataFrameQlearning.csv')\n",
    "\n",
    "# display(dfLoadQ)\n",
    "\n",
    "# Open and read the JSON file from Task1\n",
    "with open('metricsQ.json', 'r') as fileQ:\n",
    "    baseQ = json.load(fileQ)\n",
    "\n",
    "# Convert to a tuple    \n",
    "baseQTuple = (baseQ['Average Reward per Episode'], baseQ['Success Rate (%)'], baseQ['Average Learning Speed'])\n",
    "\n",
    "print(\"Performance Comparison with and without Teacher Advice for 100% Availability (Q-learning)\")\n",
    "\n",
    "# Use plot_comparison_with_baseline function from utils.py\n",
    "utils.plot_comparison_with_baseline(availability=1.0, df_learning=dfLoadQ, baseline_learning=baseQTuple, algorithm='Q-learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SARSA: loading results from Task 2 and 4, performance comparison of teacher mechanism with baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe from Task4\n",
    "dfLoadSarsa = pd.read_csv('dataFrameSarsa.csv')\n",
    "\n",
    "# display(dfLoadSarsa)\n",
    "\n",
    "# Open and read the JSON file from Task2\n",
    "with open('metricsS.json', 'r') as fileS:\n",
    "    baseS = json.load(fileS)\n",
    "\n",
    "# Convert to a tuple\n",
    "baseSTuple = (baseS['Average Reward per Episode'], baseS['Success Rate (%)'], baseS['Average Learning Speed'])\n",
    "\n",
    "print(\"Performance Comparison with and without Teacher Advice for 100% Availability (SARSA)\")\n",
    "\n",
    "# Use plot_comparison_with_baseline function from utils.py\n",
    "utils.plot_comparison_with_baseline(availability=1.0, df_learning=dfLoadSarsa, baseline_learning=baseSTuple, algorithm='SARSA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment2N",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
